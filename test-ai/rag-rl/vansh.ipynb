{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220c34a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import Optional, Union\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "import threading\n",
    "from copy import deepcopy\n",
    "import time\n",
    "import random\n",
    "from functools import wraps\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GROQ_KEY = os.getenv(\"GROQ_KEY\")\n",
    "GEMINI_KEY = os.getenv(\"GEMINI_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a98940",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url='https://api.groq.com/openai/v1',\n",
    "    api_key=GROQ_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125da734",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gemini_client = OpenAI(\n",
    "    base_url='https://generativelanguage.googleapis.com/v1beta/openai/',\n",
    "    api_key=GEMINI_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f13b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_limit_handler(max_retries: int = 5, initial_delay: float = 5.0, max_delay: float = 60.0, backoff_factor: float = 2.0, jitter: bool = True):\n",
    "    \"\"\"\n",
    "    Decorator to handle rate limiting with exponential backoff and jitter.\n",
    "    \n",
    "    Args:\n",
    "        max_retries (int): Maximum number of retry attempts. Default is 5.\n",
    "        initial_delay (float): Initial delay in seconds. Default is 1.0.\n",
    "        max_delay (float): Maximum delay in seconds. Default is 60.0.\n",
    "        backoff_factor (float): Factor by which to multiply the delay after each attempt. Default is 2.0.\n",
    "        jitter (bool): Whether to add random jitter to delays. Default is True.\n",
    "        \n",
    "    Returns:\n",
    "        Decorator function that adds rate limiting to the wrapped function.\n",
    "    \"\"\"\n",
    "    def decorator(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            delay = 5.0\n",
    "            last_exception = None\n",
    "            \n",
    "            for attempt in range(max_retries + 1):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    error_message = str(e).lower()\n",
    "                    last_exception = e\n",
    "                    \n",
    "                    rate_limit_indicators = [\n",
    "                        'rate limit', 'rate_limit', 'quota', 'too many requests', \n",
    "                        'requests per minute', 'requests per day', '429', \n",
    "                        'throttle', 'throttling', 'resource exhausted'\n",
    "                    ]\n",
    "                    \n",
    "                    is_rate_limit = any(indicator in error_message for indicator in rate_limit_indicators)\n",
    "                    \n",
    "                    is_api_error = hasattr(e, 'status_code') and e.status_code in [429, 503, 502, 500]\n",
    "                    \n",
    "                    if not (is_rate_limit or is_api_error):\n",
    "                        raise e\n",
    "                    \n",
    "                    if attempt == max_retries:\n",
    "                        print(f\"‚ùå Max retries ({max_retries}) exceeded for rate limiting. Last error: {e}\")\n",
    "                        raise e\n",
    "                    \n",
    "                    actual_delay = min(delay, max_delay)\n",
    "                    \n",
    "\n",
    "                    if jitter:\n",
    "                        jitter_amount = actual_delay * 0.1 * random.random()\n",
    "                        actual_delay += jitter_amount\n",
    "                    \n",
    "                    print(f\"‚ö†Ô∏è  Rate limit hit (attempt {attempt + 1}/{max_retries + 1}). Retrying in {actual_delay:.2f}s...\")\n",
    "                    print(f\"   Error: {e}\")\n",
    "                    \n",
    "                    time.sleep(actual_delay)\n",
    "                    delay *= backoff_factor\n",
    "            \n",
    "            raise last_exception\n",
    "        \n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20a2691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_batch_processing(items: list, batch_size: int, processing_func, delay_between_batches: float = 0.5, **kwargs):\n",
    "    \"\"\"\n",
    "    Process items in batches with safe error handling and delays to prevent rate limiting.\n",
    "    \n",
    "    Args:\n",
    "        items (list): List of items to process\n",
    "        batch_size (int): Number of items to process in each batch\n",
    "        processing_func: Function to process each batch\n",
    "        delay_between_batches (float): Delay in seconds between batches. Default is 0.5.\n",
    "        **kwargs: Additional arguments to pass to processing_func\n",
    "        \n",
    "    Returns:\n",
    "        list: Combined results from all batches\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    total_batches = (len(items) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(0, len(items), batch_size):\n",
    "        batch = items[i:i + batch_size]\n",
    "        batch_num = (i // batch_size) + 1\n",
    "        \n",
    "        print(f\"üîÑ Processing batch {batch_num}/{total_batches} ({len(batch)} items)...\")\n",
    "        \n",
    "        try:\n",
    "            batch_results = processing_func(batch, **kwargs)\n",
    "            all_results.extend(batch_results)\n",
    "\n",
    "            if batch_num < total_batches:\n",
    "                time.sleep(delay_between_batches)\n",
    "                \n",
    "        except:\n",
    "            raise\n",
    "    \n",
    "    print(f\"üéâ All {total_batches} batches completed successfully!\")\n",
    "    return all_results\n",
    "\n",
    "def check_rate_limit_status():\n",
    "    \"\"\"\n",
    "    Simple function to check if we can make API calls (useful for debugging).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        gemini_client.embeddings.create(\n",
    "            model=\"gemini-embedding-001\",\n",
    "            input=[\"test\"]\n",
    "        )\n",
    "        print(\"‚úÖ API is accessible - no rate limiting detected\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        error_message = str(e).lower()\n",
    "        if any(indicator in error_message for indicator in ['rate limit', 'quota', '429']):\n",
    "            print(f\"‚ö†Ô∏è Rate limiting detected: {e}\")\n",
    "            return False\n",
    "        else:\n",
    "            print(f\"‚ùå API error (not rate limiting): {e}\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9244d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(directory_path: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Load all text documents from the specified directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing text files.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of strings, where each string is the content of a text file.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(directory_path, filename), 'r', encoding='utf-8') as file:\n",
    "                documents.append(file.read())\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb0c4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(documents: list[str], chunk_size: int = 100, overlap: int = 20) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split documents into overlapping chunks of specified size for better context preservation.\n",
    "\n",
    "    Args:\n",
    "        documents (List[str]): A list of document strings to be split into chunks.\n",
    "        chunk_size (int): The maximum number of words in each chunk. Default is 100.\n",
    "        overlap (int): Number of words to overlap between chunks. Default is 20.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of chunks with overlapping content for better context.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for doc in documents:\n",
    "        words = doc.split()\n",
    "        for i in range(0, len(words), chunk_size - overlap):\n",
    "            chunk = \" \".join(words[i:i + chunk_size])\n",
    "            if len(chunk.strip()) > 10:\n",
    "                chunks.append(chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712724bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess the input text by removing excessive whitespace while preserving important punctuation.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        str: The preprocessed text with normalized spacing and preserved mathematical notation.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    text = re.sub(r'\\s+', ' ', text.strip()) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fb3252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_chunks(chunks: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Apply preprocessing to all text chunks.\n",
    "\n",
    "    Args:\n",
    "        chunks (List[str]): A list of text chunks to preprocess.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of preprocessed text chunks.\n",
    "    \"\"\"\n",
    "    return [preprocess_text(chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328005ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rate_limit_handler(max_retries=5, initial_delay=1.0, max_delay=60.0, backoff_factor=2.0, jitter=True)\n",
    "def generate_embeddings_batch(chunks_batch: list[str], model: str = \"gemini-embedding-001\") -> list[list[float]]:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a batch of text chunks using the Gemini client with rate limiting.\n",
    "    \n",
    "    Args:\n",
    "        chunks_batch (List[str]): A batch of text chunks to generate embeddings for.\n",
    "        model (str): The model to use for embedding generation. Default is \"gemini-embedding-001\".\n",
    "\n",
    "    Returns:\n",
    "        List[List[float]]: A list of embeddings, where each embedding is a list of floats.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = gemini_client.embeddings.create(\n",
    "            model=model,\n",
    "            input=chunks_batch\n",
    "        )\n",
    "        embeddings = [item.embedding for item in response.data]\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generate_embeddings_batch: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5b5264",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"data\"\n",
    "documents = load_documents(directory_path)\n",
    "chunks = split_into_chunks(documents)\n",
    "preprocessed_chunks = preprocess_chunks(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73a9d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(chunks: list[str], batch_size: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate embeddings for all text chunks in batches with safe rate limiting.\n",
    "\n",
    "    Args:\n",
    "        chunks (List[str]): A list of text chunks to generate embeddings for.\n",
    "        batch_size (int): The number of chunks to process in each batch. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A NumPy array containing embeddings for all chunks.\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ Starting embedding generation for {len(chunks)} chunks in batches of {batch_size}\")\n",
    "    \n",
    "    if not check_rate_limit_status():\n",
    "        print(\"‚ö†Ô∏è API issues detected, but proceeding with rate limiting protection...\")\n",
    "    \n",
    "    try:\n",
    "        all_embeddings = safe_batch_processing(\n",
    "            items=chunks,\n",
    "            batch_size=batch_size,\n",
    "            processing_func=generate_embeddings_batch,\n",
    "            delay_between_batches=0.5\n",
    "        )\n",
    "        \n",
    "        embeddings_array = np.array(all_embeddings)\n",
    "        print(f\"‚úÖ Successfully generated {embeddings_array.shape[0]} embeddings with dimension {embeddings_array.shape[1]}\")\n",
    "        return embeddings_array\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in generate_embeddings: {e}\")\n",
    "        print(\"üí° Tip: If you're hitting rate limits, try:\")\n",
    "        print(\"   - Reducing batch_size (currently {batch_size})\")\n",
    "        print(\"   - Increasing delay_between_batches\")\n",
    "        print(\"   - Check your API quota and usage\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e859a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(embeddings: np.ndarray, output_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Save embeddings to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.ndarray): A NumPy array containing the embeddings to save.\n",
    "        output_file (str): The path to the output JSON file where embeddings will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(embeddings.tolist(), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e711a72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_chunks = preprocess_chunks(chunks)\n",
    "embeddings = generate_embeddings(preprocessed_chunks)\n",
    "save_embeddings(embeddings, \"embeddings.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d1f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store: dict[int, dict[str, object]] = {}\n",
    "\n",
    "def add_to_vector_store(embeddings: np.ndarray, chunks: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Add embeddings and their corresponding text chunks to the vector store.\n",
    "\n",
    "    Args:\n",
    "        embeddings (np.ndarray): A NumPy array containing the embeddings to add.\n",
    "        chunks (List[str]): A list of text chunks corresponding to the embeddings.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for embedding, chunk in zip(embeddings, chunks):\n",
    "        vector_store[len(vector_store)] = {\n",
    "            \"embedding\": embedding, \"chunk\": chunk}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aef16b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two vectors.\n",
    "\n",
    "    Args:\n",
    "        vec1 (np.ndarray): The first vector.\n",
    "        vec2 (np.ndarray): The second vector.\n",
    "\n",
    "    Returns:\n",
    "        float: The cosine similarity between the two vectors, ranging from -1 to 1.\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb6895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_search(query_embedding: np.ndarray, top_k: int = 5) -> list[str]:\n",
    "    \"\"\"\n",
    "    Perform similarity search in the vector store and return the top_k most similar chunks.\n",
    "\n",
    "    Args:\n",
    "        query_embedding (np.ndarray): The embedding vector of the query.\n",
    "        top_k (int): The number of most similar chunks to retrieve. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of the top_k most similar text chunks.\n",
    "    \"\"\"\n",
    "    similarities = [] \n",
    "\n",
    "    for key, value in vector_store.items():\n",
    "        similarity = cosine_similarity(query_embedding, value[\"embedding\"])\n",
    "        similarities.append((key, similarity))\n",
    "\n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return [vector_store[key][\"chunk\"] for key, _ in similarities[:top_k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bc1ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_embedding_cache = {}\n",
    "\n",
    "def retrieve_relevant_chunks(query_text: str, top_k: int = 5) -> list[str]:\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant document chunks for a given query text with caching and consistent preprocessing.\n",
    "\n",
    "    Args:\n",
    "        query_text (str): The query text for which relevant chunks are to be retrieved.\n",
    "        top_k (int): The number of most relevant chunks to retrieve. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of the top_k most relevant text chunks.\n",
    "    \"\"\"\n",
    "    processed_query = preprocess_text(query_text)\n",
    "    \n",
    "    if processed_query not in _embedding_cache:\n",
    "        _embedding_cache[processed_query] = generate_embeddings([processed_query])[0]\n",
    "    query_embedding = _embedding_cache[processed_query]\n",
    "    \n",
    "    relevant_chunks = similarity_search(query_embedding, top_k=top_k)\n",
    "\n",
    "    return relevant_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f414a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_to_vector_store(embeddings, preprocessed_chunks)\n",
    "query_text = \"What is Quantum Computing?\"\n",
    "relevant_chunks = retrieve_relevant_chunks(query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7c59cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, chunk in enumerate(relevant_chunks):\n",
    "    print(f\"Chunk {idx + 1}: {chunk[:50]} ... \")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882f66d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_prompt(query: str, context_chunks: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Construct an improved prompt by combining the query with the retrieved context chunks.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query text for which the prompt is being constructed.\n",
    "        context_chunks (List[str]): A list of relevant context chunks to include in the prompt.\n",
    "\n",
    "    Returns:\n",
    "        str: The constructed prompt to be used as input for the LLM.\n",
    "    \"\"\"\n",
    "    if not context_chunks:\n",
    "        return f\"Question: {query}\\n\\nAnswer: I don't have enough information to answer this question.\"\n",
    "\n",
    "    context = \"\\n\\n\".join([f\"Context {i+1}: {chunk}\" for i, chunk in enumerate(context_chunks)])\n",
    "\n",
    "    system_message = (\n",
    "        \"You are a helpful assistant specializing in quantum computing. \"\n",
    "        \"Use the provided context to give accurate, detailed answers. \"\n",
    "        \"If the context doesn't fully address the question, clearly state what information is missing. \"\n",
    "        \"Focus on technical accuracy and include relevant mathematical expressions when appropriate.\"\n",
    "    )\n",
    "    \n",
    "    prompt = f\"System: {system_message}\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9385501",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rate_limit_handler(max_retries=5, initial_delay=1.0, max_delay=60.0, backoff_factor=2.0, jitter=True)\n",
    "def generate_response(\n",
    "    prompt: str,\n",
    "    model: str = \"llama3-70b-8192\",\n",
    "    max_tokens: int = 8000,\n",
    "    temperature: float = 0.3,\n",
    "    top_p: float = 0.9,\n",
    "    top_k: int = 50\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a response from the OpenAI chat model based on the constructed prompt with rate limiting.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input prompt to provide to the chat model.\n",
    "        model (str): The model to use for generating the response. Default is \"gemini-2.5-flash-lite\".\n",
    "        max_tokens (int): Maximum number of tokens in the response. Default is 512.\n",
    "        temperature (float): Sampling temperature for response diversity. Default is 1.\n",
    "        top_p (float): Probability mass for nucleus sampling. Default is 0.9.\n",
    "        top_k (int): Number of highest probability tokens to consider. Default is 50.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response from the chat model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            max_completion_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            # extra_body={\n",
    "            #     \"top_k\": top_k\n",
    "            # },\n",
    "            messages=[  \n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\", \n",
    "                            \"text\": prompt  \n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error in generate_response: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2171fda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_rag_pipeline(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Implement the basic Retrieval-Augmented Generation (RAG) pipeline:\n",
    "    retrieve relevant chunks, construct a prompt, and generate a response.\n",
    "\n",
    "    Args:\n",
    "        query (str): The input query for which a response is to be generated.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response from the LLM based on the query and retrieved context.\n",
    "    \"\"\"\n",
    "    relevant_chunks: list[str] = retrieve_relevant_chunks(query)\n",
    "\n",
    "    prompt: str = construct_prompt(query, relevant_chunks)\n",
    "\n",
    "    response: str = generate_response(prompt)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd00f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/val.json', 'r') as file:\n",
    "    validation_data = json.load(file)\n",
    "\n",
    "sample_query = validation_data['basic_factual_questions'][0]['question']\n",
    "expected_answer = validation_data['basic_factual_questions'][0]['answer']\n",
    "\n",
    "print(f\"Sample Query: {sample_query}\\n\")\n",
    "print(f\"Expected Answer: {expected_answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe86c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Running the Retrieval-Augmented Generation (RAG) pipeline...\")\n",
    "print(f\"üì• Query: {sample_query}\\n\")\n",
    "\n",
    "response = basic_rag_pipeline(sample_query)\n",
    "\n",
    "print(\"ü§ñ AI Response:\")\n",
    "print(\"-\" * 50)\n",
    "print(response.strip())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"‚úÖ Ground Truth Answer:\")\n",
    "print(\"-\" * 50)\n",
    "print(expected_answer)\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afb09c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_state(\n",
    "    query: str,\n",
    "    context_chunks: list[str],\n",
    "    rewritten_query: str = None,\n",
    "    previous_responses: list[str] = None,\n",
    "    previous_rewards: list[float] = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Define the state representation for the reinforcement learning agent.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The original user query.\n",
    "        context_chunks (List[str]): Retrieved context chunks from the knowledge base.\n",
    "        rewritten_query (str, optional): A reformulated version of the original query.\n",
    "        previous_responses (List[str], optional): List of previously generated responses.\n",
    "        previous_rewards (List[float], optional): List of rewards received for previous actions.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary representing the current state with all relevant information.\n",
    "    \"\"\"\n",
    "    state = {\n",
    "        \"original_query\": query,\n",
    "        \"current_query\": rewritten_query if rewritten_query else query,\n",
    "        \"context\": context_chunks,\n",
    "        \"previous_responses\": previous_responses if previous_responses else [],\n",
    "        \"previous_rewards\": previous_rewards if previous_rewards else []\n",
    "    }\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078ed44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_action_space() -> list[str]:\n",
    "    \"\"\"\n",
    "    Define the set of possible actions the reinforcement learning agent can take.\n",
    "    \n",
    "    Actions include:\n",
    "    - rewrite_query: Reformulate the original query to improve retrieval\n",
    "    - expand_context: Retrieve additional context chunks\n",
    "    - filter_context: Remove irrelevant context chunks\n",
    "    - generate_response: Generate a response based on current query and context\n",
    "    \n",
    "    Returns:\n",
    "        List[str]: A list of available actions.\n",
    "    \"\"\"\n",
    "    actions = [\"rewrite_query\", \"expand_context\",\n",
    "               \"filter_context\", \"generate_response\"]\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b5a850",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ground_truth_cache = {}\n",
    "\n",
    "def calculate_reward(response: str, ground_truth: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate a reward value by comparing the generated response to the ground truth with rate limiting protection.\n",
    "    \n",
    "    Uses cosine similarity between the embeddings of the response and ground truth\n",
    "    to determine how close the response is to the expected answer.\n",
    "    \n",
    "    Args:\n",
    "        response (str): The generated response from the RAG pipeline.\n",
    "        ground_truth (str): The expected correct answer.\n",
    "    \n",
    "    Returns:\n",
    "        float: A reward value between -1 and 1, where higher values indicate \n",
    "               greater similarity to the ground truth.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"üßÆ Calculating reward using embedding similarity...\")\n",
    "        if ground_truth not in _ground_truth_cache:\n",
    "            _ground_truth_cache[ground_truth] = generate_embeddings([ground_truth])[0]\n",
    "        response_embedding = generate_embeddings([response])[0]\n",
    "        ground_truth_embedding = _ground_truth_cache[ground_truth]\n",
    "\n",
    "        similarity = cosine_similarity(response_embedding, ground_truth_embedding)\n",
    "        print(f\"üìä Similarity score: {similarity:.4f}\")\n",
    "        return similarity\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error calculating reward with embeddings: {e}\")\n",
    "        print(\"üîÑ Falling back to simple text similarity...\")\n",
    "        \n",
    "        response_tokens = set(response.lower().split())\n",
    "        ground_truth_tokens = set(ground_truth.lower().split())\n",
    "        \n",
    "        if not response_tokens or not ground_truth_tokens:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = len(response_tokens.intersection(ground_truth_tokens))\n",
    "        union = len(response_tokens.union(ground_truth_tokens))\n",
    "        \n",
    "        jaccard_similarity = intersection / union if union > 0 else 0.0\n",
    "        print(f\"üìä Fallback Jaccard similarity: {jaccard_similarity:.4f}\")\n",
    "        return jaccard_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b98f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rate_limit_handler(max_retries=5, initial_delay=1.0, max_delay=60.0, backoff_factor=2.0, jitter=True)\n",
    "def rewrite_query(\n",
    "    query: str,\n",
    "    context_chunks: list[str],\n",
    "    model: str = \"llama3-70b-8192\",\n",
    "    max_tokens: int = 8000,\n",
    "    temperature: float = 0.3\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Use the LLM to rewrite the query for better document retrieval with rate limiting.\n",
    "\n",
    "    Args:\n",
    "        query (str): The original query text.\n",
    "        context_chunks (List[str]): A list of context chunks retrieved so far.\n",
    "        model (str): The model to use for generating the rewritten query. Default is \"gemini-2.5-flash-lite\".\n",
    "        max_tokens (int): Maximum number of tokens in the rewritten query. Default is 10000.\n",
    "        temperature (float): Sampling temperature for response diversity. Default is 0.3.\n",
    "\n",
    "    Returns:\n",
    "        str: The rewritten query optimized for document retrieval.\n",
    "    \"\"\"\n",
    "    rewrite_prompt = f\"\"\"\n",
    "    You are a query optimization assistant. Your task is to rewrite the given query to make it more effective \n",
    "    for retrieving relevant information. The query will be used for document retrieval.\n",
    "    \n",
    "    Original query: {query}\n",
    "    \n",
    "    Based on the context retrieved so far:\n",
    "    {' '.join(context_chunks[:2]) if context_chunks else 'No context available yet'}\n",
    "    \n",
    "    Rewrite the query to be more specific and targeted to retrieve better information.\n",
    "    Rewritten query:\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            max_completion_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": rewrite_prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        rewritten_query = response.choices[0].message.content.strip()\n",
    "        return rewritten_query\n",
    "    except Exception as e:\n",
    "        print(f\"Error in rewrite_query: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec14453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_context(query: str, current_chunks: list[str], top_k: int = 3) -> list[str]:\n",
    "    \"\"\"\n",
    "    Expand the context by retrieving additional chunks.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query text for which additional context is needed.\n",
    "        current_chunks (List[str]): The current list of context chunks.\n",
    "        top_k (int): The number of additional chunks to retrieve. Default is 3.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: The expanded list of context chunks including new unique chunks.\n",
    "    \"\"\"\n",
    "    additional_chunks = retrieve_relevant_chunks(\n",
    "        query, top_k=top_k + len(current_chunks))\n",
    "\n",
    "    new_chunks = []\n",
    "    for chunk in additional_chunks:\n",
    "        if chunk not in current_chunks:\n",
    "            new_chunks.append(chunk)\n",
    "\n",
    "    expanded_context = current_chunks + new_chunks[:top_k]\n",
    "    return expanded_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd71a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_context(query: str, context_chunks: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Filter the context to keep only the most relevant chunks with rate limiting protection.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query text for which relevance is calculated.\n",
    "        context_chunks (list[str]): The list of context chunks to filter.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A filtered list of the most relevant context chunks.\n",
    "    \"\"\"\n",
    "    if not context_chunks:\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        print(f\"üîç Filtering {len(context_chunks)} context chunks for relevance...\")\n",
    "        \n",
    "        if query not in _embedding_cache:\n",
    "            print(f\"üîÑ Generating embedding for filter query: {query[:50]}...\")\n",
    "            _embedding_cache[query] = generate_embeddings([query])[0]\n",
    "        else:\n",
    "            print(f\"‚úÖ Using cached query embedding for filtering...\")\n",
    "        \n",
    "        query_embedding = _embedding_cache[query]\n",
    "\n",
    "        chunk_embeddings = generate_embeddings(context_chunks)\n",
    "\n",
    "        relevance_scores = []\n",
    "        for chunk_embedding in chunk_embeddings:\n",
    "            score = cosine_similarity(query_embedding, chunk_embedding)\n",
    "            relevance_scores.append(score)\n",
    "\n",
    "        scored_chunks = list(zip(relevance_scores, context_chunks))\n",
    "        sorted_chunks = sorted(scored_chunks, key=lambda x: x[0], reverse=True)\n",
    "        filtered_chunks = [chunk for _, chunk in sorted_chunks[:min(5, len(sorted_chunks))]]\n",
    "        \n",
    "        return filtered_chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in filter_context: {e}\")\n",
    "        print(\"üîÑ Falling back to returning original chunks...\")\n",
    "        return context_chunks[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfb00cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_network(\n",
    "    state: dict,\n",
    "    action_space: list[str],\n",
    "    step_count: int = 0,\n",
    "    max_steps: int = 10,\n",
    "    epsilon: float = 0.1\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Define a policy network to select an action based on the current state using an improved strategy.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of the environment, including query, context, responses, and rewards.\n",
    "        action_space (List[str]): The list of possible actions the agent can take.\n",
    "        step_count (int): Current step number in the episode.\n",
    "        max_steps (int): Maximum steps allowed per episode.\n",
    "        epsilon (float): The probability of choosing a random action for exploration. Default is 0.1.\n",
    "\n",
    "    Returns:\n",
    "        str: The selected action from the action space.\n",
    "    \"\"\"\n",
    "    if step_count >= max_steps - 1:\n",
    "        print(f\"üèÅ Final step {step_count}, forcing response generation...\")\n",
    "        return \"generate_response\"\n",
    "    \n",
    "    if np.random.random() < epsilon:\n",
    "        action = np.random.choice(action_space)\n",
    "        print(f\"üé≤ Random exploration: chose '{action}' (step {step_count})\")\n",
    "    else:\n",
    "        if len(state[\"previous_responses\"]) == 0:\n",
    "            if len(state[\"context\"]) < 3:\n",
    "                action = \"expand_context\"\n",
    "                print(f\"üîç Insufficient context, expanding (step {step_count})\")\n",
    "            else:\n",
    "                action = \"rewrite_query\"\n",
    "                print(f\"üîÑ First iteration, rewriting query (step {step_count})\")\n",
    "        elif state[\"previous_rewards\"]:\n",
    "            latest_reward = state[\"previous_rewards\"][-1]\n",
    "            if latest_reward < 0.5:\n",
    "                if len(state[\"context\"]) > 7:\n",
    "                    action = \"filter_context\"\n",
    "                    print(f\"üîç Low reward with many contexts, filtering (step {step_count})\")\n",
    "                else:\n",
    "                    action = \"expand_context\"\n",
    "                    print(f\"üìà Low reward ({latest_reward:.3f}), expanding context (step {step_count})\")\n",
    "            elif latest_reward < 0.7:\n",
    "                action = \"rewrite_query\"\n",
    "                print(f\"üîÑ Moderate reward ({latest_reward:.3f}), rewriting query (step {step_count})\")\n",
    "            else:\n",
    "                action = \"generate_response\"\n",
    "                print(f\"‚úÖ Good reward ({latest_reward:.3f}), generating response (step {step_count})\")\n",
    "        else:\n",
    "            action = \"generate_response\"\n",
    "            print(f\"‚úÖ Default: generating response (step {step_count})\")\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af9766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rl_step(\n",
    "    state: dict,\n",
    "    action_space: list[str],\n",
    "    ground_truth: str,\n",
    "    step_count: int = 0\n",
    ") -> tuple[dict, str, float, str]:\n",
    "    \"\"\"\n",
    "    Perform a single RL step: select an action, execute it, and calculate the reward.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of the environment, including query, context, responses, and rewards.\n",
    "        action_space (List[str]): The list of possible actions the agent can take.\n",
    "        ground_truth (str): The expected correct answer to calculate the reward.\n",
    "        step_count (int): Current step number in the episode.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - state (dict): The updated state after executing the action.\n",
    "            - action (str): The action selected by the policy network.\n",
    "            - reward (float): The reward received for the action.\n",
    "            - response (str): The response generated (if applicable).\n",
    "    \"\"\"\n",
    "    action: str = policy_network(state, action_space, step_count)\n",
    "    response: str = None\n",
    "    reward: float = 0\n",
    "\n",
    "    print(f\"üéØ Executing action: {action}\")\n",
    "\n",
    "    if action == \"rewrite_query\":\n",
    "        rewritten_query: str = rewrite_query(\n",
    "            state[\"original_query\"], state[\"context\"])\n",
    "        print(f\"üîÑ Query rewritten: {rewritten_query[:100]}...\")\n",
    "        state[\"current_query\"] = rewritten_query\n",
    "        new_context: list[str] = retrieve_relevant_chunks(rewritten_query)\n",
    "        state[\"context\"] = new_context \n",
    "\n",
    "    elif action == \"expand_context\":\n",
    "        expanded_context: list[str] = expand_context(\n",
    "            state[\"current_query\"], state[\"context\"])\n",
    "        print(f\"üìà Context expanded from {len(state['context'])} to {len(expanded_context)} chunks\")\n",
    "        state[\"context\"] = expanded_context\n",
    "\n",
    "    elif action == \"filter_context\":\n",
    "        filtered_context: list[str] = filter_context(\n",
    "            state[\"current_query\"], state[\"context\"])\n",
    "        print(f\"üîç Context filtered from {len(state['context'])} to {len(filtered_context)} chunks\")\n",
    "        state[\"context\"] = filtered_context \n",
    "\n",
    "    elif action == \"generate_response\":\n",
    "        prompt: str = construct_prompt(\n",
    "            state[\"current_query\"], state[\"context\"])\n",
    "        response: str = generate_response(prompt)\n",
    "        reward: float = calculate_reward(response, ground_truth)\n",
    "        state[\"previous_responses\"].append(response)\n",
    "        state[\"previous_rewards\"].append(reward)\n",
    "        print(f\"‚úÖ Response generated with reward: {reward:.4f}\")\n",
    "\n",
    "    return state, action, reward, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70bd618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_training_params() -> dict[str, float | int]:\n",
    "    \"\"\"\n",
    "    Initialize training parameters such as learning rate, number of episodes, and discount factor.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, float | int]: A dictionary containing the initialized training parameters.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"learning_rate\": 0.001,  # Lower learning rate for more stable learning\n",
    "        \"num_episodes\": 500,      # More episodes for better learning\n",
    "        \"discount_factor\": 0.95, # Slightly lower for more immediate rewards focus\n",
    "        \"num_workers\": min(multiprocessing.cpu_count(), 4),  # Fewer workers for better control\n",
    "        \"use_threads\": True      # Use threads instead of processes for better performance\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c8550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(\n",
    "    policy: dict[str, dict[str, float | str]],\n",
    "    state: dict[str, object],\n",
    "    action: str,\n",
    "    reward: float,\n",
    "    learning_rate: float\n",
    ") -> dict[str, dict[str, float | str]]:\n",
    "    \"\"\"\n",
    "    Update the policy based on the reward received.\n",
    "\n",
    "    Args:\n",
    "        policy (dict[str, dict[str, float | str]]): The current policy to be updated.\n",
    "        state (dict[str, object]): The current state of the environment.\n",
    "        action (str): The action taken by the agent.\n",
    "        reward (float): The reward received for the action.\n",
    "        learning_rate (float): The learning rate for updating the policy.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, dict[str, float | str]]: The updated policy.\n",
    "    \"\"\"\n",
    "    query_key = state[\"current_query\"]\n",
    "    policy[query_key] = {\n",
    "        \"action\": action, \n",
    "        \"reward\": reward \n",
    "    }\n",
    "    print(f\"üìù Updated policy for query: {query_key[:50]}... with reward: {reward:.4f}\")\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb8a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_progress(\n",
    "    episode: int,\n",
    "    reward: float,\n",
    "    rewards_history: list[float]\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Track the training progress by storing rewards for each episode.\n",
    "\n",
    "    Args:\n",
    "        episode (int): The current episode number.\n",
    "        reward (float): The reward received in the current episode.\n",
    "        rewards_history (List[float]): A list to store the rewards for all episodes.\n",
    "\n",
    "    Returns:\n",
    "        List[float]: The updated rewards history.\n",
    "    \"\"\"\n",
    "    rewards_history.append(reward)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}: Reward = {reward}\")\n",
    "\n",
    "    return rewards_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11e86f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(\n",
    "    query_text: str,\n",
    "    ground_truth: str,\n",
    "    params: Optional[dict[str, Union[float, int]]] = None\n",
    ") -> tuple[dict[str, dict[str, Union[float, str]]], list[float], list[list[str]], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Implement the training loop for RL-enhanced RAG.\n",
    "\n",
    "    Args:\n",
    "        query_text (str): The input query text for the RAG pipeline.\n",
    "        ground_truth (str): The expected correct answer for the query.\n",
    "        params (Optional[dict[str, Union[float, int]]]): Training parameters such as learning rate,\n",
    "            number of episodes, and discount factor. If None, default parameters are initialized.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - policy (dict[str, dict[str, Union[float, str]]]): The updated policy after training.\n",
    "            - rewards_history (list[float]): A list of rewards received in each episode.\n",
    "            - actions_history (list[list[str]]): A list of actions taken in each episode.\n",
    "            - best_response (Optional[str]): The best response generated during training.\n",
    "    \"\"\"\n",
    "    if params is None:\n",
    "        params = initialize_training_params()\n",
    "\n",
    "    rewards_history: list[float] = [] \n",
    "    actions_history: list[list[str]] = []\n",
    "    policy: dict[str, dict[str, Union[float, str]]] = {}\n",
    "    action_space: list[str] = define_action_space() \n",
    "    best_response: Optional[str] = None \n",
    "    best_reward: float = -1 \n",
    "\n",
    "    print(\"üöÄ Starting RL-RAG training...\")\n",
    "    print(f\"üìù Query: {query_text}\")\n",
    "    print(f\"üéØ Ground truth: {ground_truth[:100]}...\")\n",
    "\n",
    "    simple_response: str = basic_rag_pipeline(query_text)\n",
    "    simple_reward: float = calculate_reward(simple_response, ground_truth)\n",
    "    print(f\"üìä Baseline Simple RAG reward: {simple_reward:.4f}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    for episode in range(params[\"num_episodes\"]):\n",
    "        print(f\"\\nüé¨ Episode {episode + 1}/{params['num_episodes']}\")\n",
    "        \n",
    "        context_chunks: list[str] = retrieve_relevant_chunks(query_text)\n",
    "        state: dict[str, object] = define_state(query_text, context_chunks)\n",
    "        episode_reward: float = 0  \n",
    "        episode_actions: list[str] = []\n",
    "\n",
    "        for step in range(10):\n",
    "            state, action, reward, response = rl_step(\n",
    "                state, action_space, ground_truth, step_count=step)\n",
    "            episode_actions.append(action)  \n",
    "\n",
    "            if response:\n",
    "                episode_reward = reward \n",
    "                print(f\"üéØ Episode {episode + 1} completed with reward: {reward:.4f}\")\n",
    "\n",
    "                if reward > best_reward:\n",
    "                    best_reward = reward\n",
    "                    best_response = response\n",
    "                    print(f\"üèÜ New best reward: {reward:.4f}\")\n",
    "                break \n",
    "\n",
    "        rewards_history.append(episode_reward)\n",
    "        actions_history.append(episode_actions)\n",
    "\n",
    "        if episode % 5 == 0 or episode == params[\"num_episodes\"] - 1:\n",
    "            print(f\"üìà Episode {episode}: Reward = {episode_reward:.4f}, Actions = {episode_actions}\")\n",
    "\n",
    "    improvement: float = best_reward - simple_reward\n",
    "    print(f\"\\nüèÅ Training completed!\")\n",
    "    print(f\"üìä Baseline Simple RAG reward: {simple_reward:.4f}\")\n",
    "    print(f\"üèÜ Best RL-enhanced RAG reward: {best_reward:.4f}\")\n",
    "    print(f\"üìà Improvement: {improvement:.4f} ({improvement * 100:.2f}%)\")\n",
    "\n",
    "    return policy, rewards_history, actions_history, best_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0826e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_episode_worker(\n",
    "    episode_id: int,\n",
    "    query_text: str, \n",
    "    ground_truth: str,\n",
    "    action_space: list[str],\n",
    "    max_steps: int = 10\n",
    ") -> tuple[int, float, list[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Worker function to execute a single RL episode in parallel.\n",
    "    \n",
    "    Args:\n",
    "        episode_id (int): Unique identifier for the episode.\n",
    "        query_text (str): The input query text for the RAG pipeline.\n",
    "        ground_truth (str): The expected correct answer for the query.\n",
    "        action_space (list[str]): The list of possible actions the agent can take.\n",
    "        max_steps (int): Maximum number of steps per episode. Default is 10.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - episode_id (int): The episode identifier.\n",
    "            - episode_reward (float): The final reward for the episode.\n",
    "            - episode_actions (list[str]): List of actions taken during the episode.\n",
    "            - response (Optional[str]): The final response generated (if any).\n",
    "    \"\"\"\n",
    "    context_chunks: list[str] = retrieve_relevant_chunks(query_text)\n",
    "    state: dict[str, object] = define_state(query_text, context_chunks)\n",
    "    episode_reward: float = 0\n",
    "    episode_actions: list[str] = []\n",
    "    response: Optional[str] = None\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        state, action, reward, step_response = rl_step(\n",
    "            state, action_space, ground_truth\n",
    "        )\n",
    "        episode_actions.append(action)\n",
    "        \n",
    "        if step_response:\n",
    "            episode_reward = reward\n",
    "            response = step_response\n",
    "            break\n",
    "    \n",
    "    return episode_id, episode_reward, episode_actions, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe4c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_rag_approaches(query_text: str, ground_truth: str) -> tuple[str, str, float, float]:\n",
    "    \"\"\"\n",
    "    Compare the outputs of simple RAG versus RL-enhanced RAG.\n",
    "\n",
    "    Args:\n",
    "        query_text (str): The input query text for the RAG pipeline.\n",
    "        ground_truth (str): The expected correct answer for the query.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[str, str, float, float]: A tuple containing:\n",
    "            - simple_response (str): The response generated by the simple RAG pipeline.\n",
    "            - best_rl_response (str): The best response generated by the RL-enhanced RAG pipeline.\n",
    "            - simple_similarity (float): The similarity score of the simple RAG response to the ground truth.\n",
    "            - rl_similarity (float): The similarity score of the RL-enhanced RAG response to the ground truth.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Query: {query_text}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    simple_response: str = basic_rag_pipeline(query_text)\n",
    "    simple_similarity: float = calculate_reward(simple_response, ground_truth)\n",
    "\n",
    "    print(\"\\nSimple RAG Output:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(simple_response)\n",
    "    print(f\"Similarity to ground truth: {simple_similarity:.4f}\")\n",
    "\n",
    "    print(\"\\nTraining RL-enhanced RAG model...\")\n",
    "    params: dict[str, float | int] = initialize_training_params()\n",
    "    params[\"num_episodes\"] = 5\n",
    "\n",
    "    _, rewards_history, actions_history, best_rl_response = training_loop(\n",
    "        query_text, ground_truth, params\n",
    "    )\n",
    "\n",
    "    if best_rl_response is None:\n",
    "        context_chunks: list[str] = retrieve_relevant_chunks(query_text) \n",
    "        prompt: str = construct_prompt(query_text, context_chunks)\n",
    "        best_rl_response: str = generate_response(prompt)\n",
    "\n",
    "    rl_similarity: float = calculate_reward(best_rl_response, ground_truth)\n",
    "\n",
    "    print(\"\\nRL-enhanced RAG Output:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(best_rl_response)\n",
    "    print(f\"Similarity to ground truth: {rl_similarity:.4f}\")\n",
    "\n",
    "    improvement: float = rl_similarity - simple_similarity\n",
    "\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Simple RAG similarity to ground truth: {simple_similarity:.4f}\")\n",
    "    print(f\"RL-enhanced RAG similarity to ground truth: {rl_similarity:.4f}\")\n",
    "    print(f\"Improvement: {improvement * 100:.2f}%\")\n",
    "\n",
    "    if len(rewards_history) > 1:\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(rewards_history)\n",
    "            plt.title('Reward History During RL Training')\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Reward')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        except ImportError:\n",
    "            print(\"Matplotlib not available for plotting rewards\")\n",
    "\n",
    "    return simple_response, best_rl_response, simple_similarity, rl_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18419fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_relevance(retrieved_chunks: list[str], ground_truth_chunks: list[str]) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the relevance of retrieved chunks by comparing them to ground truth chunks.\n",
    "\n",
    "    Args:\n",
    "        retrieved_chunks (List[str]): A list of text chunks retrieved by the system.\n",
    "        ground_truth_chunks (List[str]): A list of ground truth text chunks for comparison.\n",
    "\n",
    "    Returns:\n",
    "        float: The average relevance score between the retrieved chunks and the ground truth chunks.\n",
    "    \"\"\"\n",
    "    relevance_scores: list[float] = [\n",
    "    ]\n",
    "\n",
    "    for retrieved, ground_truth in zip(retrieved_chunks, ground_truth_chunks):\n",
    "        relevance: float = cosine_similarity(\n",
    "            generate_embeddings([retrieved])[0],\n",
    "            generate_embeddings([ground_truth])[0]\n",
    "        )\n",
    "        relevance_scores.append(relevance)\n",
    "\n",
    "    return np.mean(relevance_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265a2521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(responses: list[str], ground_truth_responses: list[str]) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of generated responses by comparing them to ground truth responses.\n",
    "\n",
    "    Args:\n",
    "        responses (List[str]): A list of generated responses to evaluate.\n",
    "        ground_truth_responses (List[str]): A list of ground truth responses to compare against.\n",
    "\n",
    "    Returns:\n",
    "        float: The average accuracy score, calculated as the mean cosine similarity \n",
    "               between the embeddings of the generated responses and the ground truth responses.\n",
    "    \"\"\"\n",
    "    accuracy_scores: list[float] = [\n",
    "    ] \n",
    "    for response, ground_truth in zip(responses, ground_truth_responses):\n",
    "        accuracy: float = cosine_similarity(\n",
    "            generate_embeddings([response])[0],\n",
    "            generate_embeddings([ground_truth])[0]\n",
    "        )\n",
    "        accuracy_scores.append(accuracy)\n",
    "\n",
    "    return np.mean(accuracy_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bcac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response_quality(responses: list[str]) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the quality of responses using a heuristic or external model.\n",
    "\n",
    "    Args:\n",
    "        responses (list[str]): A list of generated responses to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        float: The average quality score of the responses, ranging from 0 to 1.\n",
    "    \"\"\"\n",
    "    quality_scores: list[float] = [\n",
    "    ]  \n",
    "\n",
    "    for response in responses:\n",
    "        quality: float = len(response.split()) / 100\n",
    "        quality_scores.append(min(quality, 1.0))\n",
    "\n",
    "    return np.mean(quality_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a22657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_performance(\n",
    "    queries: list[str],\n",
    "    ground_truth_chunks: list[str],\n",
    "    ground_truth_responses: list[str]\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate the performance of the RAG pipeline using relevance, accuracy, and response quality metrics.\n",
    "\n",
    "    Args:\n",
    "        queries (List[str]): A list of query strings to evaluate.\n",
    "        ground_truth_chunks (List[str]): A list of ground truth text chunks corresponding to the queries.\n",
    "        ground_truth_responses (List[str]): A list of ground truth responses corresponding to the queries.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: A dictionary containing the average relevance, accuracy, and quality scores.\n",
    "    \"\"\"\n",
    "    relevance_scores: list[float] = []\n",
    "    accuracy_scores: list[float] = []\n",
    "    quality_scores: list[float] = []\n",
    "\n",
    "\n",
    "    for query, ground_truth_chunk, ground_truth_response in zip(queries, ground_truth_chunks, ground_truth_responses):\n",
    "        retrieved_chunks: list[str] = retrieve_relevant_chunks(query)\n",
    "\n",
    "        relevance: float = evaluate_relevance(\n",
    "            retrieved_chunks, [ground_truth_chunk])\n",
    "        relevance_scores.append(relevance)\n",
    "\n",
    "        response: str = basic_rag_pipeline(query)\n",
    "\n",
    "        accuracy: float = evaluate_accuracy(\n",
    "            [response], [ground_truth_response])\n",
    "        accuracy_scores.append(accuracy)\n",
    "\n",
    "        quality: float = evaluate_response_quality([response])\n",
    "        quality_scores.append(quality)\n",
    "\n",
    "    avg_relevance: float = np.mean(relevance_scores)\n",
    "    avg_accuracy: float = np.mean(accuracy_scores)\n",
    "    avg_quality: float = np.mean(quality_scores)\n",
    "\n",
    "    return {\n",
    "        \"average_relevance\": avg_relevance,\n",
    "        \"average_accuracy\": avg_accuracy,\n",
    "        \"average_quality\": avg_quality\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a2ac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Running the Retrieval-Augmented Generation (RAG) pipeline...\")\n",
    "print(f\"üì• Query: {sample_query}\\n\")\n",
    "\n",
    "response = basic_rag_pipeline(sample_query)\n",
    "\n",
    "print(\"ü§ñ AI Response:\")\n",
    "print(\"-\" * 50)\n",
    "print(response.strip())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"‚úÖ Ground Truth Answer:\")\n",
    "print(\"-\" * 50)\n",
    "print(expected_answer)\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1aee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_response, rl_response, simple_sim, rl_sim = compare_rag_approaches(\n",
    "    sample_query, expected_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-rl (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
